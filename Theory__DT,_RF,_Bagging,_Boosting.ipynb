{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MJanbandhu/Machine-Learning/blob/main/Theory__DT%2C_RF%2C_Bagging%2C_Boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDcUAXjpGqVJ"
      },
      "source": [
        "## Decision Tree\n",
        "\n",
        "Decision tree algorithm is one of the most versatile algorithms in machine learning which can perform both classification and regression analysis. It is very powerful and works great with complex datasets. Apart from that, it is very easy to understand and read. That makes it more popular to use. When coupled with ensemble techniques – which we will learn very soon- it performs even better.\n",
        "As the name suggests, this algorithm works by dividing the whole dataset into a tree-like structure based on some rules and conditions and then gives prediction based on those conditions.\n",
        "Let’s understand the approach to decision tree with a basic scenario.\n",
        "Suppose it’s Friday night and you are not able to decide if you should go out or stay at home. Let the decision tree decide it for you.\n",
        "\n",
        "\n",
        "<img src=\"Decision_tree1.PNG\" width=\"300\">\n",
        "                         \n",
        "Although we may or may not use the decision tree for such decisions, this was a basic example to help you understand how a decision tree makes a decision.\n",
        "So how did it work?\n",
        "*\tIt selects a root node based on a given condition, e.g. our root node was chosen as time >10 pm.\n",
        "*\tThen, the root node was split into child notes based on the given condition. The right child node in the above figure fulfilled the condition, so no more questions were asked.\n",
        "*\tThe left child node didn’t fulfil the condition, so again it was split based on a new condition.\n",
        "*\tThis process continues till all the conditions are met or if you have predefined the depth of your tree, e.g. the depth of our tree is 3, and it reached there when all the conditions were exhausted.\n",
        "\n",
        "Let’s see how the parent nodes and condition is chosen for the splitting to work.\n",
        "\n",
        "#### Decision Tree for Regression\n",
        "When performing regression with a decision tree, we try to divide the given values of X into distinct and non-overlapping regions, e.g. for a set of possible values X1, X2,..., Xp; we will try to divide them into J distinct and non-overlapping regions R1, R2, . . . , RJ.\n",
        "For a given observation falling into the region Rj, the prediction is equal to the mean of the response(y) values for each training observations(x) in the region Rj.\n",
        "The regions R1,R2, . . . , RJ  are selected in a way to reduce the following sum of squares of residuals :\n",
        "\n",
        "\n",
        "<img src=\"formula1.PNG\" width=\"300\">\n",
        "                                                        \n",
        "Where, yrj (second term) is the mean of all the response variables in the region ‘j’.\n",
        "\n",
        "\n",
        "\n",
        "#### Recursive binary splitting(Greedy approach)\n",
        "As mentioned above, we try to divide the X values into j regions, but it is very expensive in terms of computational time to try to fit every set of X values into j regions. Thus, decision tree opts for a top-down greedy approach in which nodes are divided into two regions based on the given condition, i.e. not every node will be split but the ones which satisfy the condition are split into two branches. It is called greedy because it does the best split at a given step at that point of time rather than looking for splitting a step for a better tree in upcoming steps. It decides a threshold value(say s) to divide the observations into different regions(j) such that the RSS for Xj>= s and Xj <s is minimum.\n",
        "\n",
        "\n",
        "<img src=\"formula2.PNG\" width=\"400\">\n",
        "                      \n",
        "Here for the above equation, j and s are found such that this equation has the minimum value.\n",
        "The regions R1, R2 are selected based on that value of s and j such that the equation above has the minimum value.\n",
        "Similarly, more regions are split out of the regions created above based on some condition with the same logic. This continues until a stopping criterion (predefined) is achieved.\n",
        "Once all the regions are split, the prediction is made based on the mean of observations in that region.\n",
        "\n",
        "The process mentioned above has a high chance of overfitting the training data as it will be very complex.\n",
        "\n",
        "\n",
        "### Classification Trees\n",
        "\n",
        "Regression trees are used for quantitative data. In the case of qualitative data or categorical data, we use classification trees.  In regression trees, we split the nodes based on RSS criteria, but in classification, it is done using classification error rate, Gini impurity and entropy.\n",
        "Let’s understand these terms in detail.\n",
        "\n",
        "#### Entropy\n",
        "Entropy is the measure of randomness in the data. In other words, it gives the impurity present in the dataset.\n",
        "\n",
        "<img src=\"entropy.PNG\" width=\"300\">\n",
        "                                           \n",
        "When we split our nodes into two regions and put different observations in both the regions, the main goal is to reduce the entropy i.e. reduce the randomness in the region and divide our data cleanly than it was in the previous node. If splitting the node doesn’t lead into entropy reduction, we try to split based on a different condition, or we stop.\n",
        "A region is clean (low entropy) when it contains data with the same labels and random if there is a mixture of labels present (high entropy).\n",
        "Let’s suppose there are ‘m’ observations and we need to classify them into categories 1 and 2.\n",
        "Let’s say that category 1 has ‘n’ observations and category 2 has ‘m-n’ observations.\n",
        "\n",
        "p= n/m  and    q = m-n/m = 1-p\n",
        "\n",
        "then, entropy for the given set is:\n",
        "\n",
        "\n",
        "          E = -p*log2(p) – q*log2(q)\n",
        "           \n",
        "           \n",
        "When all the observations belong to category 1, then p = 1 and all observations belong to category 2, then p =0, int both cases E =0, as there is no randomness in the categories.\n",
        "If half of the observations are in category 1 and another half in category 2, then p =1/2 and q =1/2, and the entropy is maximum, E =1.\n",
        "\n",
        "\n",
        "<img src=\"entropy1.PNG\" width=\"300\">\n",
        "                                  \n",
        "\n",
        "#### Information Gain\n",
        "Information gain calculates the decrease in entropy after splitting a node. It is the difference between entropies before and after the split. The more the information gain, the more entropy is removed.\n",
        "\n",
        "<img src=\"info_gain.PNG\" width=\"300\">\n",
        "\n",
        "                                 \n",
        "Where, T is the parent node before split and X is the split node from T.\n",
        "\n",
        "A tree which is splitted on basis of entropy and information gain value looks like:\n",
        "\n",
        "<img src=\"entropy_tree.PNG\" width=\"900\">\n",
        "\n",
        "#### Ginni Impurity\n",
        "According to wikipedia, ‘Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labelled if it was randomly labelled according to the distribution of labels in the subset.’\n",
        "It is calculated by multiplying the probability that a given observation is classified into the correct class and sum of all the probabilities when that particular observation is classified into the wrong class.\n",
        "Let’s suppose there are k number of classes and an observation belongs to the class ‘i’, then Ginni impurity is given as:\n",
        "\n",
        "<img src=\"ginni.PNG\" width=\"300\">\n",
        "                                    \n",
        "Ginni impurity value lies between 0 and 1, 0 being no impurity and 1 denoting random distribution.\n",
        "The node for which the Ginni impurity is least is selected as the root node to split.\n",
        "\n",
        "\n",
        "A tree which is splitted on basis of ginni impurity value looks like:\n",
        "\n",
        "<img src=\"tree_example.PNG\" width=\"900\">\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGKGzVu0GqVS"
      },
      "source": [
        "### Maths behind Decision Tree Classifier\n",
        "Before we see the python implementation of decision tree. let's first understand the math behind the decision tree classfication. We will see how all the above mentioned terms are used for splitting.\n",
        "\n",
        "We will use a simple dataset which contains information about students of different classes and gender and see whether they stay in school's hostel or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHVbisn7GqVT"
      },
      "source": [
        "This is how our data set looks like :\n",
        "\n",
        "\n",
        "<img src='data_class.PNG' width=\"200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEAo25FiGqVT"
      },
      "source": [
        "Let's try and understand how the root node is selected by calcualting gini impurity. We will use the above mentioned data.\n",
        "\n",
        "We have two features which we can use for nodes: \"Class\" and \"Gender\".\n",
        "We will calculate gini impurity for each of the features and then select that feature which has least gini impurity.\n",
        "\n",
        "Let's review the formula for calculating ginni impurity:\n",
        "\n",
        "<img src='example/gini.PNG' width=\"200\">\n",
        "\n",
        "Let's start with class, we will try to gini impurity for all different values in \"class\".\n",
        "\n",
        "<img src='example/1.PNG' width=\"500\">\n",
        "\n",
        "<img src='example/2.PNG' width=\"500\">\n",
        "\n",
        "<img src='example/3.1.PNG' width=\"500\">\n",
        "\n",
        "<img src='example/3.PNG' width=\"500\">\n",
        "\n",
        "<img src='example/4.PNG' width=\"500\">\n",
        "\n",
        "<img src='example/5.PNG' width=\"500\">\n",
        "\n",
        "<img src='example/6.PNG' width=\"500\">\n",
        "\n",
        "<img src='example/7.PNG' width=\"500\">\n",
        "\n",
        "<img src='example/8.PNG' width=\"500\">\n",
        "\n",
        "This is how our Decision tree node is selected by calculating gini impurity for each node individually.\n",
        "If the number of feautures increases, then we just need to repeat the same steps after the selection of the root node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWlyaD_4GqVU"
      },
      "source": [
        "We will try and find the root nodes for the same dataset by calculating entropy and information gain.\n",
        "\n",
        "DataSet:\n",
        "\n",
        "<img src='data_class.PNG' width=\"200\">\n",
        "\n",
        "We have two features and we will try to choose the root node by calculating the information gain by splitting each feature.\n",
        "\n",
        "Let' review the formula for entropy and information gain:\n",
        "\n",
        "<img src='example/formula_entropy.PNG' width=\"300\">\n",
        "\n",
        "<img src='example/inform_gain.PNG' width=\"300\">\n",
        "\n",
        "\n",
        "Let's start with feature \"class\" :\n",
        "\n",
        "<img src='example/9.PNG' width=\"500\">\n",
        "\n",
        "<img src='example/10.1.PNG' width=\"500\">\n",
        "\n",
        "<img src='example/11.PNG' width=\"500\">\n",
        "\n",
        "<img src='example/12.PNG' width=\"500\">\n",
        "\n",
        "<img src='example/13.PNG' width=\"500\">\n",
        "\n",
        "\n",
        "Let' see the information gain from feature \"gender\" :\n",
        "\n",
        "<img src='example/10.2.PNG' width=\"500\">\n",
        "\n",
        "<img src='example/14.PNG' width=\"500\">\n",
        "\n",
        "<img src='example/15.PNG' width=\"500\">\n",
        "\n",
        "<img src='example/16.PNG' width=\"500\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGF42IqYGqVU"
      },
      "source": [
        "### Different Algorithms for Decision Tree\n",
        "\n",
        "\n",
        "* ID3 (Iterative Dichotomiser) : It is one of the algorithms used to construct decision tree for classification. It uses Information gain as the criteria for finding the root nodes and splitting them. It only accepts categorical attributes.\n",
        "\n",
        "* C4.5 : It is an extension of ID3 algorithm, and better than ID3 as it deals both continuous and discreet values.It is also used for classfication purposes.\n",
        "\n",
        "\n",
        "* Classfication and Regression Algorithm(CART) : It is the most popular algorithm used for constructing decison trees. It uses ginni impurity as the default calculation for selecting root nodes, however one can use \"entropy\" for criteria as well. This algorithm works on both regression as well as classfication problems. We will use this algorithm in our pyhton implementation.\n",
        "\n",
        "\n",
        "Entropy and Ginni impurity can be used reversibly. It doesn't affects the result much. Although, ginni is easier to compute than entropy, since entropy has a log term calculation. That's why CART algorithm uses ginni as the default algorithm.\n",
        "\n",
        "If we plot ginni vs entropy graph, we can see there is not much difference between them:\n",
        "\n",
        "<img src=\"example/entropyVsGini.PNG\" width = \"400\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPxt4lUfGqVV"
      },
      "source": [
        "##### Advantages of Decision Tree:\n",
        "\n",
        "   * It can be used for both Regression and Classification problems.\n",
        "   * Decision Trees are very easy to grasp as the rules of splitting is clearly mentioned.\n",
        "   * Complex decision tree models are very simple when visualized. It can be understood just by visualising.\n",
        "   * Scaling and normalization are not needed.\n",
        "\n",
        "\n",
        "##### Disadvantages of Decision Tree:\n",
        "\n",
        "\n",
        "   * A small change in data can cause instability in the model because of the greedy approach.\n",
        "   * Probability of overfitting is very high for Decision Trees.\n",
        "   * It takes more time to train a decision tree model than other classification algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOhvjBIxGqVp"
      },
      "source": [
        "## RandomForest Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJzl4htBAOwI"
      },
      "source": [
        "Random Forest algorithm is an ensemble model that uses “Bagging” as the ensemble method and decision tree as the individual model. It is a learning method that works by constructing multiple decision trees and the final decision is made based on the majority of the trees and is chosen by the random forest.\n",
        "The random forest comes under supervised learning and can be used for both classification as well as regression problems. But mostly, it is used for classification problems.\n",
        "A decision tree algorithm is a tree-shaped diagram which is used to determine a course of action. In decision tree, each branch of the tree represents a possible decision, occurrence, or reaction.\n",
        "\n",
        " One of the main advantages of using Random Forest The algorithm among a lot of benefits is that it reduces the risk of overfitting and as well as the required training time. Additionally, it provides a high level of accuracy. Random Forest algorithm runs efficiently in large datasets and also produces highly accurate predictions by estimating missing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xnkuzfuAOwJ"
      },
      "source": [
        "#### How does Random forest works?\n",
        "- Step 1 - Select n (e.g. 1500) random subsets from the training set.\n",
        "- Step 2 - Train “n” decision trees. (Here, 1500 for 1 each)\n",
        "- Step 3 - Each individual tree predicts the records/candidates in the train set, independently.\n",
        "- Step 4 - Make the final predictions using the majority voting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZdIXiCVAOwJ"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PJNjtGnAOwJ"
      },
      "source": [
        "\n",
        "1. The random-forest can solve both types of problems that are classification and regression and does a decent estimation on both fronts.\n",
        "2. One of the benefits of Random Forest which exists me most is the power to handle large data sets with higher dimensionality. It can handle thousands of input variables and identify the most significant variables so it is considered as one of the dimensionality reduction methods. Furthermore, the model outputs the importance of variable, which can be a very handy feature for feature selection.\n",
        "3. It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data is missing.\n",
        "4. It has methods for balancing errors in data sets where classes are imbalanced.\n",
        "5. The capability of the above can be extended to unlabeled data, leading to unsupervised clustering, data views, and outlier detection.\n",
        "6. Random forest involves the sampling of the input data with a replacement called bootstrap sampling. Here one-third of data is not used for training and can be used for testing. These are called the OUT OF BAG samples. The Error estimated on these output bag samples is known as OUT OF BAG ERROR. The study of error estimates by out of the bag provides us evidence to show that the out of bag estimate is as accurate as using a test set of the same size as the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGQzgT2GGqVq"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2NDxMIKGqVq"
      },
      "source": [
        "* n_estimators = number of trees in the foreset\n",
        "\n",
        "* max_features =These are the maximum number of features Random Forest is allowed to try in individual tree. There are multiple options available in Python to assign maximum features\n",
        "\n",
        "* max_depth =The depth of each tree in the forest. The deeper the tree, the more splits it has and it captures more information              about the data\n",
        "\n",
        "* min_samples_split =the minimum number of samples required to split an internal node. This can vary between considering at least one sample at each node to considering all of the samples at each node\n",
        "\n",
        "* min_samples_leaf = minimum number of data points allowed in a leaf node\n",
        "* bootstrap = method for sampling data points (with or without replacement)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhmAnTrTAOwK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7k-IIWSAOwK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdyl772gAOwL"
      },
      "source": [
        "# what is bagging\n",
        "* Bagging, also known as bootstrap aggregation, is the ensemble learning method that is commonly used to reduce variance within a noisy dataset. In bagging, a random sample of data in a training set is selected with replacement—meaning that the individual data points can be chosen more than once\n",
        "\n",
        "\n",
        "* Bagging is used when the goal is to reduce the variance of a decision tree classifier. Here the objective is to create several subsets of data from training sample chosen randomly with replacement. Each collection of subset data is used to train\n",
        "\n",
        "![](bag.png)\n",
        "\n",
        "\n",
        "**Output side called as  Aggregation**\n",
        "\n",
        "**For regression task it will take average**\n",
        "\n",
        "\n",
        "\n",
        "**For classification it will count the output**\n",
        "\n",
        "## How bagging works\n",
        "\n",
        "#### Bootstrapping:\n",
        "*  Bagging leverages a bootstrapping sampling technique to create diverse samples. This resampling method generates different subsets of the training dataset by selecting data points at random and with replacement. This means that each time you select a data point from the training dataset, you are able to select the same instance multiple times. As a result, a value/instance repeated twice (or more) in a sample.\n",
        "\n",
        "#### Parallel training:\n",
        "* These bootstrap samples are then trained independently and in parallel with each other using weak or base learners.\n",
        "\n",
        "#### Aggregation:\n",
        "* Finally, depending on the task (i.e. regression or classification), an average or a majority of the predictions are taken to compute a more accurate estimate. In the case of regression, an average is taken of all the outputs predicted by the individual classifiers; this is known as soft voting. For classification problems, the class with the highest majority of votes is accepted; this is known as hard voting or majority voting.\n",
        "\n",
        "## Benefits :\n",
        "\n",
        "* The biggest advantage of bagging is that multiple weak learners can work better than a single strong learner.\n",
        "\n",
        "#### Ease of implementation:\n",
        "* Python libraries such as scikit-learn (also known as sklearn) make it easy to combine the predictions of base learners or estimators to improve model performance.\n",
        "\n",
        "\n",
        "#### Reduction of variance:\n",
        "* Bagging can reduce the variance within a learning algorithm. This is particularly helpful with high-dimensional data, where missing values can lead to higher variance, making it more prone to overfitting and preventing accurate generalization to new datasets.\n",
        "\n",
        "\n",
        "## challenges of bagging:\n",
        "\n",
        "\n",
        "#### Computationally expensive:\n",
        "* Bagging slows down and grows more intensive as the number of iterations increase. Clustered systems or a large number of processing cores are ideal for quickly creating bagged ensembles on large test sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j43dyim5AOwL"
      },
      "source": [
        "# Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAbTr4WiAOwL"
      },
      "source": [
        "# What is GB_XGB ?\n",
        "* XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. A wide range of applications: Can be used to solve regression, classification\n",
        "\n",
        "\n",
        "\n",
        "* XGBoost is an implementation of gradient boosted decision trees designed for speed and performance\n",
        "\n",
        "\n",
        "\n",
        "* Boosting is an ensemble technique where new models are added to correct the errors made by existing models. Models are added sequentially until no further improvements can be made.\n",
        "\n",
        "\n",
        "\n",
        "* Gradient boosting is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction. It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models.\n",
        "\n",
        "\n",
        "\n",
        "* This approach supports both regression and classification predictive modeling problems.\n",
        "\n",
        "\n",
        "## Decision tree,Bagging,Random forest,Boosting,Gradient Boosting,XGBoost\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "\n",
        "**Why does XGBoost perform so well?**\n",
        "* XGBoost and Gradient Boosting Machines  are both ensemble tree methods that apply the principle of boosting weak           learners using the gradient descent architecture. However, XGBoost improves upon the base GBM frameworkthrough systems optimization and algorithmic enhancements.\n",
        "\n",
        "\n",
        "#### 1.Regularization:\n",
        "* This is considered to be as a dominant factor of the algorithm. Regularization is a technique that is used to get rid of overfitting of the model.\n",
        "\n",
        "#### 2.Cross-Validation:\n",
        "* We use cross-validation by importing the function from sklearn but XGboost is enabled with inbuilt CV function.\n",
        "\n",
        "#### 3.Missing Value:  \n",
        "* It is designed in such a way that it can handle missing values. It finds out the trends in the missing values and apprehends them.\n",
        "\n",
        "#### 4.Flexibility:\n",
        "* It gives the support to objective functions. They are the function used to evaluate the performance of the model and also it can handle the user-defined validation metrics.\n",
        "\n",
        "\n",
        "\n",
        "## System Optimization\n",
        "\n",
        "#### Parallelization:\n",
        "* XGBoost approaches the process of sequential tree building using parallelized implementation. This is possible due to the interchangeable nature of loops used for building base learners; the outer loop that enumerates the leaf nodes of a tree, and the second inner loop that calculates the features. This nesting of loops limits parallelization because without completing the inner loop (more computationally demanding of the two), the outer loop cannot be started. Therefore, to improve run time, the order of loops is interchanged using initialization through a global scan of all instances and sorting using parallel threads. This switch improves algorithmic performance by offsetting any parallelization overheads in computation.\n",
        "\n",
        "#### Tree Pruning:\n",
        "* The stopping criterion for tree splitting within GBM framework is greedy in nature and depends on the negative loss criterion at the point of split. XGBoost uses ‘max_depth’ parameter as specified instead of criterion first, and starts pruning trees backward. This ‘depth-first’ approach improves computational performance significantly.\n",
        "\n",
        "\n",
        "#### Hardware Optimization:\n",
        "* This algorithm has been designed to make efficient use of hardware resources. This is accomplished by cache awareness by allocating internal buffers in each thread to store gradient statistics. Further enhancements such as ‘out-of-core’ computing optimize available disk space while handling big data-frames that do not fit into memory.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**When to Use XGBoost?**\n",
        "\n",
        "* 1> When you have large number of observations in training data.**\n",
        "\n",
        "* 2> Number features < number of observations in training data.**\n",
        "\n",
        "* 3> It performs well when data has mixture numerical and categorical features or just numeric features.**\n",
        "\n",
        "* 4> When the model performance metrics are to be considered.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CaFNXoYAOwM"
      },
      "source": [
        "## XGBoost\n",
        "### Pros\n",
        "1. Less feature engineering required (No need for scaling, normalizing data, can also handle missing values well)\n",
        "2. Feature importance can be found out(it output importance of each feature, can be used for feature selection)\n",
        "3. Outliers have minimal impact.\n",
        "4. Handles large sized datasets well.\n",
        "5. Good Execution speed\n",
        "6. Good model performance (wins most of the Kaggle competitions)\n",
        "7. Less prone to overfitting\n",
        "\n",
        "### Cons\n",
        "1. Difficult interpretation , visualization tough\n",
        "2. Overfitting possible if parameters not tuned proper\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQQHpApyAOwM"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYZDeVCAAOwM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}